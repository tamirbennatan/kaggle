{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, PredefinedSplit\n",
    "\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A first XGboost model\n",
    "\n",
    "Here I train and tune an XGboost model on the first set of features I've extracted. These features are very basic, and don't involve any great insight, so I don't expect the results to be competitive. This notebook mainly serves as a baseline. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data\n",
    "\n",
    "Loading the training and test data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load(\"../data/first_features/X.npy\")\n",
    "y = np.load(\"../data/first_features/y.npy\")\n",
    "X_test =  np.load(\"../data/first_features/X_test.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1503424, 107)\n",
      "(1503424,)\n",
      "(508438, 107)\n"
     ]
    }
   ],
   "source": [
    "# sanity check - do the shapes line up? \n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can change the type of these matrices to save some memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1286930944"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.nbytes # 1.2 gig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "643465472"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.astype(np.float32).nbytes #.64 gig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.astype(np.float32)\n",
    "X_test = X_test.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Set up train/validation splits\n",
    "\n",
    "For the purpose of hyperparameter tuning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of actually splitting up the data `X` into seperate training and vaidation splits, I'll just save the indecies of the training and validation sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_idx = np.repeat(-1, y.shape)\n",
    "np.random.seed(109)\n",
    "validation_idx[np.random.choice(validation_idx.shape[0], \n",
    "       int(round(.15*validation_idx.shape[0])), replace = False)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225514\n",
      "0.15000026605934189\n"
     ]
    }
   ],
   "source": [
    "# now, we have an integer vector, where `-1` is training, and `0` is validation\n",
    "print(np.sum(validation_idx == 0))\n",
    "print(np.mean(validation_idx == 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Approximate model complexity\n",
    "\n",
    "The first thing I'll do is get a feel for how complex a model I should be using. to do this, I'll define a grid with the parameters `n_estimators` and `max_depth` with very large intervals - to get a feel for the order of magnitude I should be using. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid1 = {\"n_estimators\" : [500, 1500, 5000], \n",
    "        \"max_depth\" : [6, 18, 56]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, create a predefined validation split to be based to a `GridSearchCV` object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_split = list(PredefinedSplit(validation_idx).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = XGBRegressor(random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs1 = GridSearchCV(model1, grid1,\n",
    "           n_jobs=-1, \n",
    "           cv=2,\n",
    "           verbose=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NTS: started 11:15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 9 candidates, totalling 18 fits\n",
      "[CV] max_depth=6, n_estimators=500 ...................................\n",
      "[CV] max_depth=6, n_estimators=500 ...................................\n",
      "[CV] max_depth=6, n_estimators=1500 ..................................\n",
      "[CV] max_depth=6, n_estimators=1500 ..................................\n"
     ]
    }
   ],
   "source": [
    "gs1.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
