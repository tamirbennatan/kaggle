{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Python Class for Encoding High Cardinality Categorical Features\n",
    "\n",
    "\n",
    "### Introduction\n",
    "\n",
    "In my experience working on Kaggle Competitions reading others' solutions, it's come to my attention that there a lot of time is being spent on encoding categorical features. This preprocessing step is necessary in almost all Kaggle competitions, and so I thought I'd write a simple python class with a consistent API, so that people can spend less time encoding, and more time doing more interesting stuff. \n",
    "\n",
    "In this notebook I discuss the three types of encodings I've implemented - **One-hot Encoding**, **Frequency Encoding**, and **Mean/Label/Likelihood Encoding**. I then perform a quick case study to show how to use the class, and how effective each method works. \n",
    "\n",
    "### Why  Encode High Cardinality Categorical Features? \n",
    "\n",
    "Most machine learning toolkits require that the input training/test data be in a numeric format. The canonical way of encoding categorical features (which cannot be represented as numbers with meaningful magnitude and order), is to use *one-hot encoding*. \n",
    "\n",
    "When a categorical feature has a large number of levels, however, one-hot encoding can lead to very sparse data, with many features - even more so if you want to consider the interactions between categorical feature in question and the other features in your dataset. This often leads to a propensity to overfit to your data, as well as slower training. \n",
    "\n",
    "To overcome this, Kaggler's often use other methods of converting categorical features into numeric datatypes, but _of lower dimension_ than yielded by one-hot encoding. This allows one to use the information in a categorical feature, but avoid the troubles of high dimensional, sparse encodings. \n",
    "\n",
    "### Implemented encoding schemes\n",
    "\n",
    "In This class, I've implementd three encoding schemes (so far): **One-hot Encoding**, **Frequency Encoding**, and **Mean/Label/Likelihood Encoding**.\n",
    "\n",
    "##### One-hot encoding\n",
    "\n",
    "Categorical features are respresented as binary vectors. Each element of a vector signifies if the corresponding example belongs to a particular class. Every vector has only one element with the value `1`, and the rest are `0`. \n",
    "\n",
    "##### Frequency Encoding\n",
    "\n",
    "Here, each category is mapped to the frequency with which it appears in the training set. Thus, a categorical variable (or a group of categorical features) with any number of levels can be represented as a single numeric feature. \n",
    "\n",
    "Frequency encoding works well when the frequency of a class provides true signal of the target, and when categories of similar frequency have similar properties with respect to the target. It's a straightforward method which often offers slightly improved performance. \n",
    "\n",
    "##### Mean/Label/Likelihood\n",
    "\n",
    "I first encountered Mean encodings in the Coursera course **How to win a data science competiton**. The authors of this course claim that this method is often the key to outperforming other competitors. \n",
    "\n",
    "Mean encoding represents each category level as the average value of the response for that category. One cannot use the response value associated with a training example to encode said training example, however, as this would lead to leaking information from the response vector to the training data, and cause overfitting. \n",
    "\n",
    "Therefore, folding schemes are often implemented. A training set is split into `k` folds, where the encodings of each fold are determined by averaging the response values of the categories in the remaining `k-1` folds. This folding process is often repeated recursively, to further reduce the risk of overfitting. \n",
    "\n",
    "I felt that the details on how to implement this scheme were unclear in the course, however, and that many others agreed with me. As such, I though it would be useful to implement it once and for all, for myself and others to use. \n",
    "\n",
    "\n",
    "### What's Next? \n",
    "\n",
    "There are still many feature encoding schemes that I have read about that I would like to implement. As I continue working on Kaggle competitions/notebooks, If I find myself implementing one of these schemes, I'll add it to this class (so look out for updates!)\n",
    "\n",
    "Some of these schemes are:\n",
    "\n",
    "- Ridge Regression Feature Encodings\n",
    "- Feature Hashing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Class\n",
    "\n",
    "Below is the code for the Category Encoder class. After the code, I demonstrate the class's API, and show an example of how to use it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder, scale\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "class CategoryEncoder(object):\n",
    "    \n",
    "    @staticmethod\n",
    "    def _to_hashable(X):\n",
    "        # if X is an array, return as is\n",
    "        try:\n",
    "            if len(X.shape) == 1:\n",
    "                return(X.astype(str))\n",
    "        except AttributeError:\n",
    "            # X is not a numpy array \n",
    "            raise ValueError(\"Input must be numpy array-like, with `shape` attribute.\")\n",
    "        \"\"\"\n",
    "        Given a 2D numpy array, convert it into a 1D numpy array\n",
    "        by converting the elements into strings and joining with an underscore. \n",
    "        This makes each combination of several features hashable. \n",
    "        \n",
    "        input:\n",
    "            X: numpy 2D array\n",
    "        \"\"\"\n",
    "        # make sure X is a 2D array\n",
    "        try:\n",
    "            assert len(X.shape) == 2\n",
    "        except AttributeError:\n",
    "            # X is not a numpy array \n",
    "            raise ValueError(\"Input must be numpy array-like, with `shape` attribute.\")\n",
    "        # reformat and return\n",
    "        X_hashable = np.apply_along_axis(lambda row: \"_\".join(str(e) for e in row),\n",
    "                                         axis = 1, arr = X)\n",
    "        return(X_hashable)\n",
    "\n",
    "    @classmethod\n",
    "    def _validate_input_both(cls, X_train, X_test, y_train):\n",
    "        \"\"\"\n",
    "        Validate that X_train and X_test have the same shape, and\n",
    "        that they are either 1D or 2D nd-array types. \n",
    "        Convert X_train and X_test to 2D arrays, and y_train to 1D array. \n",
    "        \n",
    "        Should be called only when X_train and X_test are provided. \n",
    "        \n",
    "        input:\n",
    "            - X_train: Numpy Nd-Array. Each column represent a high cardinality\n",
    "                categorical variable, and each row is a training example\n",
    "                \n",
    "            - X_test:  Numpy Nd-Array. Each column represents the same high cardinality\n",
    "                categorical variable as in the training example. Warning - if the test\n",
    "                set contains factor levels not present in the training set, unexpected\n",
    "                behavior will occur.\n",
    "                \n",
    "            - y_train: Numpy Nd-Array. Target (response) varible. Should be a numeric type, \n",
    "                so that calling `y_train.mean()` makes sense. \n",
    "        \n",
    "        returns:\n",
    "            - X_train_transformed: reshaped version of X_train. The levels of the\n",
    "                different categorical variables across the features presented as input\n",
    "                are concatenated as strings to create a single, even higher cardinality\n",
    "                input. \n",
    "                \n",
    "            - X_test_transformed:  reshaped version of X_test\n",
    "            \n",
    "            - y_train_transformed: values of y_train unchanged, but returned as a numpy\n",
    "                array with shape (nrows,)\n",
    "            \n",
    "        raises:\n",
    "            - ValueError Exception\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # validate that X_train and X_test have the same shape\n",
    "            if (len(X_train.shape) != len(X_test.shape)):\n",
    "                raise ValueError(\"`X_train` and `X_test` must have the same number of dimensions\")\n",
    "        except AttributeError:\n",
    "            \"\"\"\n",
    "            If X_train or X_test are not numpy array-like\n",
    "            \"\"\"\n",
    "            raise ValueError(\"Input must be numpy array-like, with `shape` attribute.\")\n",
    "        # validate that input are of maximum 2 dimensions for X\n",
    "        if (len(X_train.shape) > 2):\n",
    "            raise ValueError(\"X input shape should be of maximum 2 dimensions\")\n",
    "            \n",
    "        # validate that y input can be represented as a numeric 1D array\n",
    "        try:\n",
    "            y_flattented = y_train.reshape(-1).astype(np.float32) # reshaped response\n",
    "            if(len(y_flattented) != len(y_train)):\n",
    "                raise ValueError(\"y_train should be able to be naturally represented as a 1D array.\")\n",
    "        except ValueError:\n",
    "            raise ValueError(\"y_train must be of a datatype that can be converted to a numeric datatype naturally.\")\n",
    "        \n",
    "        # Reshape X_train and X_test\n",
    "        if(len(X_train.shape) == 2):\n",
    "            # Convert 1D array to 1D array of hashable type\n",
    "            X_train = cls._to_hashable(X_train)\n",
    "            X_test = cls._to_hashable(X_test)\n",
    "        \n",
    "        # make sure X_train and y_train have the same length\n",
    "        if(len(X_train) != len(y_flattented)):\n",
    "            raise ValueError(\"X_train and y_train must have the same number of elements.\")\n",
    "\n",
    "        return(np.array(X_train), np.array(X_test), np.array(y_flattented))\n",
    "    \n",
    "    @classmethod\n",
    "    def _validate_input(cls, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Validate that X_train is either 1D or 2D nd-array type. \n",
    "        Convert X_train to 2D array, and y_train to 1D array. \n",
    "\n",
    "        \n",
    "        input:\n",
    "            - X_train: Numpy Nd-Array. Each column represent a high cardinality\n",
    "                categorical variable, and each row is a training example\n",
    "                \n",
    "            - y_train: Numpy Nd-Array. Target (response) varible. Should be a numeric type, \n",
    "                so that calling `y_train.mean()` makes sense. \n",
    "        \n",
    "        returns:\n",
    "            - X_train_transformed: reshaped version of X_train. The levels of the\n",
    "                different categorical variables across the features presented as input\n",
    "                are concatenated as strings to create a single, even higher cardinality\n",
    "                input. \n",
    "            \n",
    "            - y_train_transformed: values of y_train unchanged, but returned as a numpy\n",
    "                array with shape (nrows,)\n",
    "            \n",
    "        raises:\n",
    "            - ValueError Exception\n",
    "        \"\"\"\n",
    "        # validate that input are of maximum 2 dimensions for X\n",
    "        try:\n",
    "            if (len(X_train.shape) > 2):\n",
    "                raise ValueError(\"X input shape should be of maximum 2 dimensions\")\n",
    "        except AttributeError:\n",
    "            \"\"\"\n",
    "            If X_train doesn't have the `shape` attribute, then it is not\n",
    "            array-like\n",
    "            \"\"\"\n",
    "            raise ValueError(\"Input must be numpy array-like, with `shape` attribute.\")\n",
    "            \n",
    "        # validate that y input can be represented as a numeric 1D array\n",
    "        try:\n",
    "            y_flattented = y_train.reshape(-1).astype(np.float32) # reshaped response\n",
    "            if(len(y_flattented) != len(y_train)):\n",
    "                raise ValueError(\"y_train should be able to be naturally represented as a 1D array.\")\n",
    "        except ValueError:\n",
    "            raise ValueError(\"y_train must be of a datatype that can be converted to a 1D numeric datatype naturally.\")\n",
    "        \n",
    "        # Reshape X_train\n",
    "        if(len(X_train.shape) == 2):\n",
    "            # Convert 1D array to 1D array of hashable type\n",
    "            X_train = cls._to_hashable(X_train)\n",
    "        \n",
    "        # make sure X_train and y_train have the same length\n",
    "        if(len(X_train) != len(y_flattented)):\n",
    "            raise ValueError(\"X_train and y_train must have the same number of elements.\")\n",
    "\n",
    "        return(np.array(X_train), np.array(y_flattented))\n",
    "    \n",
    "    \"\"\"\n",
    "    Recursively mean encode the categories in the training data. \n",
    "    Not to be used by the outside - called by the instance method `mean_encode()`\n",
    "    \"\"\"\n",
    "    def _mean_encode_train(self, X_train, y_train, depth = 2, n_splits = 5, shuffle = True, random_state = 1):\n",
    "        # base case - depth = 1\n",
    "        if (depth == 1):\n",
    "            # create a template for the mean encoded ouptput\n",
    "            y_train_enc = np.repeat(-1.0,len(y_train))\n",
    "            # iterate through the different folds\n",
    "            kf = KFold(n_splits = n_splits, shuffle = shuffle, random_state = random_state)\n",
    "            for train_index, test_index in kf.split(X_train):\n",
    "                # compute the group means in the training folds\n",
    "                train_means = pd.DataFrame({\"group\" : X_train[train_index], \n",
    "                                            \"mean_target\": y_train[train_index]}\n",
    "                                          ).groupby(\"group\", as_index = False).mean()\n",
    "\n",
    "                # use group means to match each group with an encoding\n",
    "                encoded_means = pd.DataFrame({\"group\": X_train[test_index]}).reset_index().merge(train_means, how = \"left\")\n",
    "                \"\"\"\n",
    "                Groups that do not appear in the train fold, but do in the test fold, \n",
    "                will have NA as that particular group mean. \n",
    "                As such, fill these NA values with the global mean for the target in \n",
    "                the train fold. \n",
    "                \"\"\"\n",
    "                encoded_means = encoded_means.fillna(encoded_means.mean_target.mean())\n",
    "                # store the encoded means\n",
    "                y_train_enc[test_index] = encoded_means.sort_index().mean_target.values\n",
    "            # return the encoded target\n",
    "            return(y_train_enc)\n",
    "        \n",
    "        # Recurive step - if depth >= 2\n",
    "        else:\n",
    "            # create a template for the mean encoded ouptput\n",
    "            y_train_enc = np.repeat(-1.0,len(y_train))\n",
    "            # iterate through the different folds, encoding the training output fold by fold\n",
    "            kf = KFold(n_splits = n_splits, shuffle = shuffle, random_state = random_state)\n",
    "            for train_index, test_index in kf.split(X_train):\n",
    "                y_train_enc[test_index] = self._mean_encode_train(X_train[test_index], \n",
    "                                            y_train[test_index], depth = depth - 1, n_splits = n_splits, \n",
    "                                            shuffle = shuffle, random_state = random_state)\n",
    "            \"\"\"\n",
    "            Now, use the encoded ouptuts to 're-encode' the output again\n",
    "            \"\"\"\n",
    "            # get a new k-fold object with a different seed, which will result in different splits\n",
    "            kf = KFold(n_splits = n_splits, shuffle = shuffle, random_state = random_state + 1)\n",
    "            # template for re-encoded output\n",
    "            y_train_reencoded = np.repeat(-1.0,len(y_train))\n",
    "            for train_index, test_index in kf.split(X_train):\n",
    "                # compute the group means in the training folds\n",
    "                train_means = pd.DataFrame({\"group\" : X_train[train_index], \n",
    "                                            \"mean_target\": y_train_enc[train_index]}\n",
    "                                          ).groupby(\"group\", as_index = False).mean()\n",
    "\n",
    "                # use group means to match each group with an encoding\n",
    "                encoded_means = pd.DataFrame({\"group\": X_train[test_index]}).reset_index().merge(\n",
    "                    train_means, how = \"left\").sort_index()\n",
    "                # fill na's that result from groups in test fold that don't appear in training folds\n",
    "                encoded_means = encoded_means.fillna(encoded_means.mean_target.mean())\n",
    "                # store the encoded means\n",
    "                y_train_reencoded[test_index] = encoded_means.mean_target.values\n",
    "            return(y_train_reencoded)\n",
    "    \n",
    "    \"\"\"\n",
    "    Main instance method for mean encoding. \n",
    "    \"\"\"\n",
    "    def mean_encode(self, X_train, y_train, X_test = None, depth = 2, \n",
    "                    n_splits = 5, shuffle = True, random_state = 1):\n",
    "        \"\"\"\n",
    "        Mean encode high frequency categorical features. \n",
    "        Mean encoding is a process in which categories are re-encoded by the average\n",
    "        response value they have in a seperate holdout set. In this way, mean encoded\n",
    "        features are similar to the predictions of a KNN classifier/regressor. \n",
    "        \n",
    "        To avoid overfitting, the response any training example should not be used\n",
    "        to encode that example. This is why k-folding is used - the data is split into\n",
    "        `n_splits` folds, and the values of each fold are computed by averaging the \n",
    "        remaining `n_splits` folds. \n",
    "        \n",
    "        To further avoid overfitting, this process is repeated recursively; the depth of\n",
    "        this recursion is controlled by the parameter `depth` (default: 2)\n",
    "        \n",
    "        ----------\n",
    "        Parameters\n",
    "        ----------\n",
    "        - X_train: Numpy Nd-Array. Each column represent a high cardinality\n",
    "                categorical variable, and each row is a training example\n",
    "\n",
    "        - y_train: Numpy Nd-Array. Target (response) varible. Should be a numeric type, \n",
    "                so that calling `y_train.mean()` makes sense.  \n",
    "            \n",
    "        - X_test (default = None) :  Numpy Nd-Array. Each column represents the same high cardinality\n",
    "            categorical variable as in the training example. Warning - if the test\n",
    "            set contains factor levels not present in the training set, unexpected\n",
    "            behavior will occur.\n",
    "        \n",
    "        - depth (default = 2): Integer. Number of times to recursively use k-folding to indroduce noise to the\n",
    "            encodings, and avoid overfitting.\n",
    "            \n",
    "        - n_splits (default = 5): Integer. Number of folds (`k`) to use when using k-folding\n",
    "        \n",
    "        - shuffle (default = True): Boolean. Whether to use randomly split folds. \n",
    "        \n",
    "        - random_state (default = 1): Integer. For reproducability. \n",
    "        \n",
    "        \n",
    "        ----------\n",
    "        Returns\n",
    "        ----------\n",
    "        \n",
    "        - X_train_encoded: numeric numpy array. Encoded values of training categories. \n",
    "        \n",
    "        - X_test_encoded: numeric numpy array. Only returned if `X_test` is provided \n",
    "            (not None) when method is called.\n",
    "        \"\"\"\n",
    "        # first: Is X_test provided? \n",
    "        if X_test is not None:\n",
    "            # validate the input. \n",
    "            X_train, X_test, y_train = self._validate_input_both(X_train, X_test, y_train)  \n",
    "        else:\n",
    "            X_train, y_train = self._validate_input(X_train, y_train)\n",
    "\n",
    "        \"\"\"\n",
    "        Once inputs are validated, recursively encode the training data\n",
    "        \"\"\"\n",
    "        y_train_encoded = self._mean_encode_train(X_train, y_train, depth, n_splits, shuffle, random_state)\n",
    "        # if test set is not provided, return encoded output\n",
    "        if X_test is None:\n",
    "            return(y_train_encoded)\n",
    "        else:\n",
    "            \"\"\"\n",
    "            Need to group categories together. Multiple high cardinality factors may have been provided,\n",
    "            so need to first combine them to one even higher cardinality factor. \n",
    "            \n",
    "            Then, test encodings are the average of the train encodings within that group. \n",
    "            \"\"\"\n",
    "            # get the average encoded values for each group in the training set\n",
    "            group_averages = pd.DataFrame({\"group\":X_train, \"encoded_values\":y_train_encoded}).groupby(\n",
    "                \"group\", as_index = False).mean()\n",
    "            # join to get test encodings\n",
    "            test_encodings = pd.DataFrame({\"group\":X_test}).reset_index().merge(group_averages, how = \"left\").fillna(\n",
    "                group_averages.encoded_values.mean()).sort_index()\n",
    "            y_test_encoded = test_encodings.encoded_values.values\n",
    "            # return both training and test encoded values\n",
    "            return(y_train_encoded, y_test_encoded)\n",
    "        \n",
    "    def frequency_encode(self, X_train, X_test = None):\n",
    "        \"\"\"\n",
    "        Encode categories into a single numeric variable, corresponding to the frequency of said \n",
    "        categories. \n",
    "        If a test set is provided `X_test`, then the frequencies are determined from the training\n",
    "        set, so that classifiers trained on the test set generalize properly to the test set. \n",
    "        \n",
    "        ----------\n",
    "        Parameters\n",
    "        ----------\n",
    "        - X_train: Numpy Nd-Array. Each column represent a high cardinality\n",
    "                categorical variable, and each row is a training example\n",
    "            \n",
    "        - X_test (default = None) :  Numpy Nd-Array. Each column represents the same high cardinality\n",
    "            categorical variable as in the training example. Warning - if the test\n",
    "            set contains factor levels not present in the training set, unexpected\n",
    "            behavior will occur.\n",
    "            \n",
    "        ----------\n",
    "        Returns\n",
    "        ----------\n",
    "        \n",
    "        - X_train_encoded: numeric numpy array. Frequency encoded values of training categories. \n",
    "        \n",
    "        - X_test_encoded: numeric numpy array. Only returned if `X_test` is provided \n",
    "            (not None) when method is called.\n",
    "        \"\"\"\n",
    "        # first: Is X_test provided? \n",
    "        if X_test is not None:\n",
    "            # collapse multiple factors\n",
    "            X_train = self._to_hashable(X_train)\n",
    "            X_test = self._to_hashable(X_test)\n",
    "        else:\n",
    "            X_train = self._to_hashable(X_train)\n",
    "        # encode the training set with frequency counts\n",
    "        X_train_encoded = pd.DataFrame({\"group\":X_train}).reset_index()\n",
    "        # compute a frequency table\n",
    "        train_frequency = pd.DataFrame({\"frequency\":X_train_encoded.groupby(\"group\").size()/len(X_train_encoded)})\n",
    "        #join to get training frequency\n",
    "        X_train_encoded = X_train_encoded.merge(train_frequency, left_on = \"group\", \n",
    "                                                right_index=True,sort=False).sort_index()\n",
    "        \"\"\"\n",
    "        If no test set was provided, return the training frequencies as is.\n",
    "        Otherwise, return the test frequencies. \n",
    "        \"\"\"\n",
    "        if X_test is None:\n",
    "            return X_train_encoded.frequency.values\n",
    "        else:\n",
    "            # merge to get the test set frequencies\n",
    "            X_test_encoded = pd.DataFrame({\"group\":X_test}).reset_index()\n",
    "            X_test_encoded = X_test_encoded.merge(train_frequency, how = \"left\", left_on = \"group\", \n",
    "                                                 right_index = True, sort = False).sort_index()\n",
    "            # fill NAN values with the mean encoding \n",
    "            X_test_encoded = X_test_encoded.fillna(X_test_encoded.frequency.mean())\n",
    "            return X_train_encoded.frequency.values, X_test_encoded.frequency.values\n",
    "    \n",
    "    \n",
    "    def _label_encode(self, X_train, X_test = None):\n",
    "        \"\"\"\n",
    "        Given an array representing a categorical feature, transform into an integer array, \n",
    "        where each integer represents an level\n",
    "        \"\"\"\n",
    "        # initialize a label encoder object\n",
    "        encoder = LabelEncoder()\n",
    "        \"\"\"\n",
    "        If test set is provided, want to train on both the training labels\n",
    "        and the test labels\n",
    "        \"\"\"\n",
    "        if X_test is not None:\n",
    "            all_categories = np.append(X_train,X_test)\n",
    "        else:\n",
    "            all_categories = X_train\n",
    "        # train the encoder on the categories\n",
    "        encoder.fit(all_categories.astype(str))\n",
    "        \"\"\"\n",
    "        If test set is provided, return both transformed sets. Otherwise, only transform the\n",
    "        training set and return\n",
    "        \"\"\"\n",
    "        if X_test is not None:\n",
    "            X_train_transformed = encoder.transform(X_train)\n",
    "            X_test_transformed = encoder.transform(X_test)\n",
    "            return X_train_transformed, X_test_transformed, encoder.classes_\n",
    "        else:\n",
    "            return encoder.transform(X_train), encoder.classes_\n",
    "    \n",
    "    def onehot_encode(self, X_train, X_test = None, prefix = \"dummy_\"):\n",
    "        \"\"\"\n",
    "        Given an array represent a categorical feature, transform into a matrix of\n",
    "        one-hot vectors, encoding the feature. \n",
    "        \n",
    "        This matrix is returned as Pandas DataFrame, so that column names can help identify \n",
    "        the original category levels\n",
    "        \n",
    "        ----------\n",
    "        Parameters\n",
    "        ----------\n",
    "        - X_train: Numpy Nd-Array. Each column represent a high cardinality\n",
    "                categorical variable, and each row is a training example\n",
    "            \n",
    "        - X_test (default = None) :  Numpy Nd-Array. Each column represents the same high cardinality\n",
    "            categorical variable as in the training example. Warning - if the test\n",
    "            set contains factor levels not present in the training set, unexpected\n",
    "            behavior will occur.\n",
    "            \n",
    "        - prefix (default = \"dummy\"): Used to prefix the column names of the returned dataframe. The category levels \n",
    "            are appended to the prefix to form the column names.\n",
    "            \n",
    "            \n",
    "        ----------\n",
    "        Returns\n",
    "        ----------\n",
    "        \n",
    "        - X_train_encoded: Pandas DataFrame containing the one-hot encoded examples in the training set. \n",
    "        \n",
    "        - X_test_encoded: Pandas DataFrame containing the one-hot encoded examples in the test set. \n",
    "            only returned if X_test is provided to the method.\n",
    "        \n",
    "        \"\"\"\n",
    "        # collapse levels into a single vector\n",
    "        X_train = self._to_hashable(X_train)\n",
    "        if X_test is not None:\n",
    "            X_test = self._to_hashable(X_test)\n",
    "        # convert the classes into integer labels\n",
    "        if X_test is not None:\n",
    "            X_train_labels, X_test_labels, classes = self._label_encode(X_train, X_test)\n",
    "        else:\n",
    "            X_train_labels, classes = self._label_encode(X_train)\n",
    "        # Expand the labels into one-hot encodings\n",
    "        X_train_onehot = to_categorical(X_train_labels).astype(int)\n",
    "        if X_test is not None:\n",
    "            X_test_onehot = to_categorical(X_test_labels, num_classes = len(classes)).astype(int)\n",
    "        \"\"\"\n",
    "        Return the result(s) as dataframe(s). \n",
    "        First, build up a list of the column names to use. Then return as a dataframe,\n",
    "        where each row is the one-hot encoding of an element in X_train\n",
    "        \"\"\"\n",
    "        colnames = [prefix + c for c in classes]\n",
    "        X_train_encoded = pd.DataFrame(X_train_onehot, columns = colnames)\n",
    "        if X_test is None:\n",
    "            return X_train_encoded\n",
    "        else:\n",
    "            X_test_encoded = pd.DataFrame(X_test_onehot, columns = colnames)\n",
    "            return X_train_encoded, X_test_encoded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class API\n",
    "\n",
    "The methods in this class follow a simple pattern - If only a training set is provided upon method call, then only encodings of the training set are returned. If the test set is also provided, both the encodings of the training and test set are returned. \n",
    "\n",
    "For example, consider the following example of creating frequency encodings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training encoding:  [0.33333333 0.33333333 0.33333333 0.16666667 0.16666667 0.33333333]\n",
      "\n",
      "\n",
      "training encodings:[0.33333333 0.33333333 0.33333333 0.16666667 0.16666667 0.33333333]\n",
      "test encodings:    [0.33333333 0.33333333 0.16666667]\n"
     ]
    }
   ],
   "source": [
    "# create vector representing a categorical veraible in the training set...\n",
    "train_categories = np.array([\"green\", \"red\", \"green\", \"blue\", \"yellow\", \"red\"])\n",
    "# ... and the same categorical variable in the test set\n",
    "test_categories = np.array([\"red\", \"green\", \"yellow\"])\n",
    "\n",
    "# Initialize a category encoder\n",
    "encoder = CategoryEncoder()\n",
    "\n",
    "# Provide just the \"training set\" to encode just the forst vector:\n",
    "print(\"training encoding: \", encoder.frequency_encode(train_categories))\n",
    "print()\n",
    "\n",
    "# Or provide both the training sets and test sets to encode both\n",
    "print(\"\"\"\n",
    "training encodings:%s\n",
    "test encodings:    %s\"\"\" % encoder.frequency_encode(train_categories, test_categories))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! Just a couple lines of code, which save you from writing a couple more (and looking up how Pandas `merge` works again...)\n",
    "\n",
    "For reference on how to use this class, pleas look at:\n",
    "```python\n",
    "help(CategoryEncoder())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some more libraries\n",
    "import plotnine\n",
    "from plotnine import *\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the training and test sets\n",
    "train = pd.read_csv(\"../data/application_train.csv\")\n",
    "test = pd.read_csv(\"../data/application_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SK_ID_CURR</th>\n",
       "      <th>TARGET</th>\n",
       "      <th>NAME_CONTRACT_TYPE</th>\n",
       "      <th>CODE_GENDER</th>\n",
       "      <th>FLAG_OWN_CAR</th>\n",
       "      <th>FLAG_OWN_REALTY</th>\n",
       "      <th>CNT_CHILDREN</th>\n",
       "      <th>AMT_INCOME_TOTAL</th>\n",
       "      <th>AMT_CREDIT</th>\n",
       "      <th>AMT_ANNUITY</th>\n",
       "      <th>AMT_GOODS_PRICE</th>\n",
       "      <th>NAME_TYPE_SUITE</th>\n",
       "      <th>NAME_INCOME_TYPE</th>\n",
       "      <th>NAME_EDUCATION_TYPE</th>\n",
       "      <th>NAME_FAMILY_STATUS</th>\n",
       "      <th>NAME_HOUSING_TYPE</th>\n",
       "      <th>REGION_POPULATION_RELATIVE</th>\n",
       "      <th>DAYS_BIRTH</th>\n",
       "      <th>DAYS_EMPLOYED</th>\n",
       "      <th>DAYS_REGISTRATION</th>\n",
       "      <th>DAYS_ID_PUBLISH</th>\n",
       "      <th>OWN_CAR_AGE</th>\n",
       "      <th>FLAG_MOBIL</th>\n",
       "      <th>FLAG_EMP_PHONE</th>\n",
       "      <th>FLAG_WORK_PHONE</th>\n",
       "      <th>FLAG_CONT_MOBILE</th>\n",
       "      <th>FLAG_PHONE</th>\n",
       "      <th>FLAG_EMAIL</th>\n",
       "      <th>OCCUPATION_TYPE</th>\n",
       "      <th>CNT_FAM_MEMBERS</th>\n",
       "      <th>REGION_RATING_CLIENT</th>\n",
       "      <th>REGION_RATING_CLIENT_W_CITY</th>\n",
       "      <th>WEEKDAY_APPR_PROCESS_START</th>\n",
       "      <th>HOUR_APPR_PROCESS_START</th>\n",
       "      <th>REG_REGION_NOT_LIVE_REGION</th>\n",
       "      <th>REG_REGION_NOT_WORK_REGION</th>\n",
       "      <th>LIVE_REGION_NOT_WORK_REGION</th>\n",
       "      <th>REG_CITY_NOT_LIVE_CITY</th>\n",
       "      <th>REG_CITY_NOT_WORK_CITY</th>\n",
       "      <th>LIVE_CITY_NOT_WORK_CITY</th>\n",
       "      <th>ORGANIZATION_TYPE</th>\n",
       "      <th>EXT_SOURCE_1</th>\n",
       "      <th>EXT_SOURCE_2</th>\n",
       "      <th>EXT_SOURCE_3</th>\n",
       "      <th>APARTMENTS_AVG</th>\n",
       "      <th>BASEMENTAREA_AVG</th>\n",
       "      <th>YEARS_BEGINEXPLUATATION_AVG</th>\n",
       "      <th>YEARS_BUILD_AVG</th>\n",
       "      <th>COMMONAREA_AVG</th>\n",
       "      <th>ELEVATORS_AVG</th>\n",
       "      <th>ENTRANCES_AVG</th>\n",
       "      <th>FLOORSMAX_AVG</th>\n",
       "      <th>FLOORSMIN_AVG</th>\n",
       "      <th>LANDAREA_AVG</th>\n",
       "      <th>LIVINGAPARTMENTS_AVG</th>\n",
       "      <th>LIVINGAREA_AVG</th>\n",
       "      <th>NONLIVINGAPARTMENTS_AVG</th>\n",
       "      <th>NONLIVINGAREA_AVG</th>\n",
       "      <th>APARTMENTS_MODE</th>\n",
       "      <th>BASEMENTAREA_MODE</th>\n",
       "      <th>YEARS_BEGINEXPLUATATION_MODE</th>\n",
       "      <th>YEARS_BUILD_MODE</th>\n",
       "      <th>COMMONAREA_MODE</th>\n",
       "      <th>ELEVATORS_MODE</th>\n",
       "      <th>ENTRANCES_MODE</th>\n",
       "      <th>FLOORSMAX_MODE</th>\n",
       "      <th>FLOORSMIN_MODE</th>\n",
       "      <th>LANDAREA_MODE</th>\n",
       "      <th>LIVINGAPARTMENTS_MODE</th>\n",
       "      <th>LIVINGAREA_MODE</th>\n",
       "      <th>NONLIVINGAPARTMENTS_MODE</th>\n",
       "      <th>NONLIVINGAREA_MODE</th>\n",
       "      <th>APARTMENTS_MEDI</th>\n",
       "      <th>BASEMENTAREA_MEDI</th>\n",
       "      <th>YEARS_BEGINEXPLUATATION_MEDI</th>\n",
       "      <th>YEARS_BUILD_MEDI</th>\n",
       "      <th>COMMONAREA_MEDI</th>\n",
       "      <th>ELEVATORS_MEDI</th>\n",
       "      <th>ENTRANCES_MEDI</th>\n",
       "      <th>FLOORSMAX_MEDI</th>\n",
       "      <th>FLOORSMIN_MEDI</th>\n",
       "      <th>LANDAREA_MEDI</th>\n",
       "      <th>LIVINGAPARTMENTS_MEDI</th>\n",
       "      <th>LIVINGAREA_MEDI</th>\n",
       "      <th>NONLIVINGAPARTMENTS_MEDI</th>\n",
       "      <th>NONLIVINGAREA_MEDI</th>\n",
       "      <th>FONDKAPREMONT_MODE</th>\n",
       "      <th>HOUSETYPE_MODE</th>\n",
       "      <th>TOTALAREA_MODE</th>\n",
       "      <th>WALLSMATERIAL_MODE</th>\n",
       "      <th>EMERGENCYSTATE_MODE</th>\n",
       "      <th>OBS_30_CNT_SOCIAL_CIRCLE</th>\n",
       "      <th>DEF_30_CNT_SOCIAL_CIRCLE</th>\n",
       "      <th>OBS_60_CNT_SOCIAL_CIRCLE</th>\n",
       "      <th>DEF_60_CNT_SOCIAL_CIRCLE</th>\n",
       "      <th>DAYS_LAST_PHONE_CHANGE</th>\n",
       "      <th>FLAG_DOCUMENT_2</th>\n",
       "      <th>FLAG_DOCUMENT_3</th>\n",
       "      <th>FLAG_DOCUMENT_4</th>\n",
       "      <th>FLAG_DOCUMENT_5</th>\n",
       "      <th>FLAG_DOCUMENT_6</th>\n",
       "      <th>FLAG_DOCUMENT_7</th>\n",
       "      <th>FLAG_DOCUMENT_8</th>\n",
       "      <th>FLAG_DOCUMENT_9</th>\n",
       "      <th>FLAG_DOCUMENT_10</th>\n",
       "      <th>FLAG_DOCUMENT_11</th>\n",
       "      <th>FLAG_DOCUMENT_12</th>\n",
       "      <th>FLAG_DOCUMENT_13</th>\n",
       "      <th>FLAG_DOCUMENT_14</th>\n",
       "      <th>FLAG_DOCUMENT_15</th>\n",
       "      <th>FLAG_DOCUMENT_16</th>\n",
       "      <th>FLAG_DOCUMENT_17</th>\n",
       "      <th>FLAG_DOCUMENT_18</th>\n",
       "      <th>FLAG_DOCUMENT_19</th>\n",
       "      <th>FLAG_DOCUMENT_20</th>\n",
       "      <th>FLAG_DOCUMENT_21</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_HOUR</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_DAY</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_WEEK</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_MON</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_QRT</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_YEAR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100002</td>\n",
       "      <td>1</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>202500.0</td>\n",
       "      <td>406597.5</td>\n",
       "      <td>24700.5</td>\n",
       "      <td>351000.0</td>\n",
       "      <td>Unaccompanied</td>\n",
       "      <td>Working</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Single / not married</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>0.018801</td>\n",
       "      <td>-9461</td>\n",
       "      <td>-637</td>\n",
       "      <td>-3648.0</td>\n",
       "      <td>-2120</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Laborers</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>WEDNESDAY</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Business Entity Type 3</td>\n",
       "      <td>0.083037</td>\n",
       "      <td>0.262949</td>\n",
       "      <td>0.139376</td>\n",
       "      <td>0.0247</td>\n",
       "      <td>0.0369</td>\n",
       "      <td>0.9722</td>\n",
       "      <td>0.6192</td>\n",
       "      <td>0.0143</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0690</td>\n",
       "      <td>0.0833</td>\n",
       "      <td>0.1250</td>\n",
       "      <td>0.0369</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.0190</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0252</td>\n",
       "      <td>0.0383</td>\n",
       "      <td>0.9722</td>\n",
       "      <td>0.6341</td>\n",
       "      <td>0.0144</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0690</td>\n",
       "      <td>0.0833</td>\n",
       "      <td>0.1250</td>\n",
       "      <td>0.0377</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.0198</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0250</td>\n",
       "      <td>0.0369</td>\n",
       "      <td>0.9722</td>\n",
       "      <td>0.6243</td>\n",
       "      <td>0.0144</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0690</td>\n",
       "      <td>0.0833</td>\n",
       "      <td>0.1250</td>\n",
       "      <td>0.0375</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0193</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>reg oper account</td>\n",
       "      <td>block of flats</td>\n",
       "      <td>0.0149</td>\n",
       "      <td>Stone, brick</td>\n",
       "      <td>No</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1134.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100003</td>\n",
       "      <td>0</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>270000.0</td>\n",
       "      <td>1293502.5</td>\n",
       "      <td>35698.5</td>\n",
       "      <td>1129500.0</td>\n",
       "      <td>Family</td>\n",
       "      <td>State servant</td>\n",
       "      <td>Higher education</td>\n",
       "      <td>Married</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>0.003541</td>\n",
       "      <td>-16765</td>\n",
       "      <td>-1188</td>\n",
       "      <td>-1186.0</td>\n",
       "      <td>-291</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Core staff</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>MONDAY</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>School</td>\n",
       "      <td>0.311267</td>\n",
       "      <td>0.622246</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0959</td>\n",
       "      <td>0.0529</td>\n",
       "      <td>0.9851</td>\n",
       "      <td>0.7960</td>\n",
       "      <td>0.0605</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.0345</td>\n",
       "      <td>0.2917</td>\n",
       "      <td>0.3333</td>\n",
       "      <td>0.0130</td>\n",
       "      <td>0.0773</td>\n",
       "      <td>0.0549</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>0.0098</td>\n",
       "      <td>0.0924</td>\n",
       "      <td>0.0538</td>\n",
       "      <td>0.9851</td>\n",
       "      <td>0.8040</td>\n",
       "      <td>0.0497</td>\n",
       "      <td>0.0806</td>\n",
       "      <td>0.0345</td>\n",
       "      <td>0.2917</td>\n",
       "      <td>0.3333</td>\n",
       "      <td>0.0128</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.0554</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0968</td>\n",
       "      <td>0.0529</td>\n",
       "      <td>0.9851</td>\n",
       "      <td>0.7987</td>\n",
       "      <td>0.0608</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.0345</td>\n",
       "      <td>0.2917</td>\n",
       "      <td>0.3333</td>\n",
       "      <td>0.0132</td>\n",
       "      <td>0.0787</td>\n",
       "      <td>0.0558</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>0.01</td>\n",
       "      <td>reg oper account</td>\n",
       "      <td>block of flats</td>\n",
       "      <td>0.0714</td>\n",
       "      <td>Block</td>\n",
       "      <td>No</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-828.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100004</td>\n",
       "      <td>0</td>\n",
       "      <td>Revolving loans</td>\n",
       "      <td>M</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>67500.0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>6750.0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>Unaccompanied</td>\n",
       "      <td>Working</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Single / not married</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>0.010032</td>\n",
       "      <td>-19046</td>\n",
       "      <td>-225</td>\n",
       "      <td>-4260.0</td>\n",
       "      <td>-2531</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Laborers</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>MONDAY</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Government</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.555912</td>\n",
       "      <td>0.729567</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-815.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100006</td>\n",
       "      <td>0</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>312682.5</td>\n",
       "      <td>29686.5</td>\n",
       "      <td>297000.0</td>\n",
       "      <td>Unaccompanied</td>\n",
       "      <td>Working</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Civil marriage</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>0.008019</td>\n",
       "      <td>-19005</td>\n",
       "      <td>-3039</td>\n",
       "      <td>-9833.0</td>\n",
       "      <td>-2437</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Laborers</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>WEDNESDAY</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Business Entity Type 3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.650442</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-617.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100007</td>\n",
       "      <td>0</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>121500.0</td>\n",
       "      <td>513000.0</td>\n",
       "      <td>21865.5</td>\n",
       "      <td>513000.0</td>\n",
       "      <td>Unaccompanied</td>\n",
       "      <td>Working</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Single / not married</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>0.028663</td>\n",
       "      <td>-19932</td>\n",
       "      <td>-3038</td>\n",
       "      <td>-4311.0</td>\n",
       "      <td>-3458</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Core staff</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>THURSDAY</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Religion</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.322738</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1106.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SK_ID_CURR  TARGET NAME_CONTRACT_TYPE CODE_GENDER FLAG_OWN_CAR  \\\n",
       "0      100002       1         Cash loans           M            N   \n",
       "1      100003       0         Cash loans           F            N   \n",
       "2      100004       0    Revolving loans           M            Y   \n",
       "3      100006       0         Cash loans           F            N   \n",
       "4      100007       0         Cash loans           M            N   \n",
       "\n",
       "  FLAG_OWN_REALTY  CNT_CHILDREN  AMT_INCOME_TOTAL  AMT_CREDIT  AMT_ANNUITY  \\\n",
       "0               Y             0          202500.0    406597.5      24700.5   \n",
       "1               N             0          270000.0   1293502.5      35698.5   \n",
       "2               Y             0           67500.0    135000.0       6750.0   \n",
       "3               Y             0          135000.0    312682.5      29686.5   \n",
       "4               Y             0          121500.0    513000.0      21865.5   \n",
       "\n",
       "   AMT_GOODS_PRICE NAME_TYPE_SUITE NAME_INCOME_TYPE  \\\n",
       "0         351000.0   Unaccompanied          Working   \n",
       "1        1129500.0          Family    State servant   \n",
       "2         135000.0   Unaccompanied          Working   \n",
       "3         297000.0   Unaccompanied          Working   \n",
       "4         513000.0   Unaccompanied          Working   \n",
       "\n",
       "             NAME_EDUCATION_TYPE    NAME_FAMILY_STATUS  NAME_HOUSING_TYPE  \\\n",
       "0  Secondary / secondary special  Single / not married  House / apartment   \n",
       "1               Higher education               Married  House / apartment   \n",
       "2  Secondary / secondary special  Single / not married  House / apartment   \n",
       "3  Secondary / secondary special        Civil marriage  House / apartment   \n",
       "4  Secondary / secondary special  Single / not married  House / apartment   \n",
       "\n",
       "   REGION_POPULATION_RELATIVE  DAYS_BIRTH  DAYS_EMPLOYED  DAYS_REGISTRATION  \\\n",
       "0                    0.018801       -9461           -637            -3648.0   \n",
       "1                    0.003541      -16765          -1188            -1186.0   \n",
       "2                    0.010032      -19046           -225            -4260.0   \n",
       "3                    0.008019      -19005          -3039            -9833.0   \n",
       "4                    0.028663      -19932          -3038            -4311.0   \n",
       "\n",
       "   DAYS_ID_PUBLISH  OWN_CAR_AGE  FLAG_MOBIL  FLAG_EMP_PHONE  FLAG_WORK_PHONE  \\\n",
       "0            -2120          NaN           1               1                0   \n",
       "1             -291          NaN           1               1                0   \n",
       "2            -2531         26.0           1               1                1   \n",
       "3            -2437          NaN           1               1                0   \n",
       "4            -3458          NaN           1               1                0   \n",
       "\n",
       "   FLAG_CONT_MOBILE  FLAG_PHONE  FLAG_EMAIL OCCUPATION_TYPE  CNT_FAM_MEMBERS  \\\n",
       "0                 1           1           0        Laborers              1.0   \n",
       "1                 1           1           0      Core staff              2.0   \n",
       "2                 1           1           0        Laborers              1.0   \n",
       "3                 1           0           0        Laborers              2.0   \n",
       "4                 1           0           0      Core staff              1.0   \n",
       "\n",
       "   REGION_RATING_CLIENT  REGION_RATING_CLIENT_W_CITY  \\\n",
       "0                     2                            2   \n",
       "1                     1                            1   \n",
       "2                     2                            2   \n",
       "3                     2                            2   \n",
       "4                     2                            2   \n",
       "\n",
       "  WEEKDAY_APPR_PROCESS_START  HOUR_APPR_PROCESS_START  \\\n",
       "0                  WEDNESDAY                       10   \n",
       "1                     MONDAY                       11   \n",
       "2                     MONDAY                        9   \n",
       "3                  WEDNESDAY                       17   \n",
       "4                   THURSDAY                       11   \n",
       "\n",
       "   REG_REGION_NOT_LIVE_REGION  REG_REGION_NOT_WORK_REGION  \\\n",
       "0                           0                           0   \n",
       "1                           0                           0   \n",
       "2                           0                           0   \n",
       "3                           0                           0   \n",
       "4                           0                           0   \n",
       "\n",
       "   LIVE_REGION_NOT_WORK_REGION  REG_CITY_NOT_LIVE_CITY  \\\n",
       "0                            0                       0   \n",
       "1                            0                       0   \n",
       "2                            0                       0   \n",
       "3                            0                       0   \n",
       "4                            0                       0   \n",
       "\n",
       "   REG_CITY_NOT_WORK_CITY  LIVE_CITY_NOT_WORK_CITY       ORGANIZATION_TYPE  \\\n",
       "0                       0                        0  Business Entity Type 3   \n",
       "1                       0                        0                  School   \n",
       "2                       0                        0              Government   \n",
       "3                       0                        0  Business Entity Type 3   \n",
       "4                       1                        1                Religion   \n",
       "\n",
       "   EXT_SOURCE_1  EXT_SOURCE_2  EXT_SOURCE_3  APARTMENTS_AVG  BASEMENTAREA_AVG  \\\n",
       "0      0.083037      0.262949      0.139376          0.0247            0.0369   \n",
       "1      0.311267      0.622246           NaN          0.0959            0.0529   \n",
       "2           NaN      0.555912      0.729567             NaN               NaN   \n",
       "3           NaN      0.650442           NaN             NaN               NaN   \n",
       "4           NaN      0.322738           NaN             NaN               NaN   \n",
       "\n",
       "   YEARS_BEGINEXPLUATATION_AVG  YEARS_BUILD_AVG  COMMONAREA_AVG  \\\n",
       "0                       0.9722           0.6192          0.0143   \n",
       "1                       0.9851           0.7960          0.0605   \n",
       "2                          NaN              NaN             NaN   \n",
       "3                          NaN              NaN             NaN   \n",
       "4                          NaN              NaN             NaN   \n",
       "\n",
       "   ELEVATORS_AVG  ENTRANCES_AVG  FLOORSMAX_AVG  FLOORSMIN_AVG  LANDAREA_AVG  \\\n",
       "0           0.00         0.0690         0.0833         0.1250        0.0369   \n",
       "1           0.08         0.0345         0.2917         0.3333        0.0130   \n",
       "2            NaN            NaN            NaN            NaN           NaN   \n",
       "3            NaN            NaN            NaN            NaN           NaN   \n",
       "4            NaN            NaN            NaN            NaN           NaN   \n",
       "\n",
       "   LIVINGAPARTMENTS_AVG  LIVINGAREA_AVG  NONLIVINGAPARTMENTS_AVG  \\\n",
       "0                0.0202          0.0190                   0.0000   \n",
       "1                0.0773          0.0549                   0.0039   \n",
       "2                   NaN             NaN                      NaN   \n",
       "3                   NaN             NaN                      NaN   \n",
       "4                   NaN             NaN                      NaN   \n",
       "\n",
       "   NONLIVINGAREA_AVG  APARTMENTS_MODE  BASEMENTAREA_MODE  \\\n",
       "0             0.0000           0.0252             0.0383   \n",
       "1             0.0098           0.0924             0.0538   \n",
       "2                NaN              NaN                NaN   \n",
       "3                NaN              NaN                NaN   \n",
       "4                NaN              NaN                NaN   \n",
       "\n",
       "   YEARS_BEGINEXPLUATATION_MODE  YEARS_BUILD_MODE  COMMONAREA_MODE  \\\n",
       "0                        0.9722            0.6341           0.0144   \n",
       "1                        0.9851            0.8040           0.0497   \n",
       "2                           NaN               NaN              NaN   \n",
       "3                           NaN               NaN              NaN   \n",
       "4                           NaN               NaN              NaN   \n",
       "\n",
       "   ELEVATORS_MODE  ENTRANCES_MODE  FLOORSMAX_MODE  FLOORSMIN_MODE  \\\n",
       "0          0.0000          0.0690          0.0833          0.1250   \n",
       "1          0.0806          0.0345          0.2917          0.3333   \n",
       "2             NaN             NaN             NaN             NaN   \n",
       "3             NaN             NaN             NaN             NaN   \n",
       "4             NaN             NaN             NaN             NaN   \n",
       "\n",
       "   LANDAREA_MODE  LIVINGAPARTMENTS_MODE  LIVINGAREA_MODE  \\\n",
       "0         0.0377                  0.022           0.0198   \n",
       "1         0.0128                  0.079           0.0554   \n",
       "2            NaN                    NaN              NaN   \n",
       "3            NaN                    NaN              NaN   \n",
       "4            NaN                    NaN              NaN   \n",
       "\n",
       "   NONLIVINGAPARTMENTS_MODE  NONLIVINGAREA_MODE  APARTMENTS_MEDI  \\\n",
       "0                       0.0                 0.0           0.0250   \n",
       "1                       0.0                 0.0           0.0968   \n",
       "2                       NaN                 NaN              NaN   \n",
       "3                       NaN                 NaN              NaN   \n",
       "4                       NaN                 NaN              NaN   \n",
       "\n",
       "   BASEMENTAREA_MEDI  YEARS_BEGINEXPLUATATION_MEDI  YEARS_BUILD_MEDI  \\\n",
       "0             0.0369                        0.9722            0.6243   \n",
       "1             0.0529                        0.9851            0.7987   \n",
       "2                NaN                           NaN               NaN   \n",
       "3                NaN                           NaN               NaN   \n",
       "4                NaN                           NaN               NaN   \n",
       "\n",
       "   COMMONAREA_MEDI  ELEVATORS_MEDI  ENTRANCES_MEDI  FLOORSMAX_MEDI  \\\n",
       "0           0.0144            0.00          0.0690          0.0833   \n",
       "1           0.0608            0.08          0.0345          0.2917   \n",
       "2              NaN             NaN             NaN             NaN   \n",
       "3              NaN             NaN             NaN             NaN   \n",
       "4              NaN             NaN             NaN             NaN   \n",
       "\n",
       "   FLOORSMIN_MEDI  LANDAREA_MEDI  LIVINGAPARTMENTS_MEDI  LIVINGAREA_MEDI  \\\n",
       "0          0.1250         0.0375                 0.0205           0.0193   \n",
       "1          0.3333         0.0132                 0.0787           0.0558   \n",
       "2             NaN            NaN                    NaN              NaN   \n",
       "3             NaN            NaN                    NaN              NaN   \n",
       "4             NaN            NaN                    NaN              NaN   \n",
       "\n",
       "   NONLIVINGAPARTMENTS_MEDI  NONLIVINGAREA_MEDI FONDKAPREMONT_MODE  \\\n",
       "0                    0.0000                0.00   reg oper account   \n",
       "1                    0.0039                0.01   reg oper account   \n",
       "2                       NaN                 NaN                NaN   \n",
       "3                       NaN                 NaN                NaN   \n",
       "4                       NaN                 NaN                NaN   \n",
       "\n",
       "   HOUSETYPE_MODE  TOTALAREA_MODE WALLSMATERIAL_MODE EMERGENCYSTATE_MODE  \\\n",
       "0  block of flats          0.0149       Stone, brick                  No   \n",
       "1  block of flats          0.0714              Block                  No   \n",
       "2             NaN             NaN                NaN                 NaN   \n",
       "3             NaN             NaN                NaN                 NaN   \n",
       "4             NaN             NaN                NaN                 NaN   \n",
       "\n",
       "   OBS_30_CNT_SOCIAL_CIRCLE  DEF_30_CNT_SOCIAL_CIRCLE  \\\n",
       "0                       2.0                       2.0   \n",
       "1                       1.0                       0.0   \n",
       "2                       0.0                       0.0   \n",
       "3                       2.0                       0.0   \n",
       "4                       0.0                       0.0   \n",
       "\n",
       "   OBS_60_CNT_SOCIAL_CIRCLE  DEF_60_CNT_SOCIAL_CIRCLE  DAYS_LAST_PHONE_CHANGE  \\\n",
       "0                       2.0                       2.0                 -1134.0   \n",
       "1                       1.0                       0.0                  -828.0   \n",
       "2                       0.0                       0.0                  -815.0   \n",
       "3                       2.0                       0.0                  -617.0   \n",
       "4                       0.0                       0.0                 -1106.0   \n",
       "\n",
       "   FLAG_DOCUMENT_2  FLAG_DOCUMENT_3  FLAG_DOCUMENT_4  FLAG_DOCUMENT_5  \\\n",
       "0                0                1                0                0   \n",
       "1                0                1                0                0   \n",
       "2                0                0                0                0   \n",
       "3                0                1                0                0   \n",
       "4                0                0                0                0   \n",
       "\n",
       "   FLAG_DOCUMENT_6  FLAG_DOCUMENT_7  FLAG_DOCUMENT_8  FLAG_DOCUMENT_9  \\\n",
       "0                0                0                0                0   \n",
       "1                0                0                0                0   \n",
       "2                0                0                0                0   \n",
       "3                0                0                0                0   \n",
       "4                0                0                1                0   \n",
       "\n",
       "   FLAG_DOCUMENT_10  FLAG_DOCUMENT_11  FLAG_DOCUMENT_12  FLAG_DOCUMENT_13  \\\n",
       "0                 0                 0                 0                 0   \n",
       "1                 0                 0                 0                 0   \n",
       "2                 0                 0                 0                 0   \n",
       "3                 0                 0                 0                 0   \n",
       "4                 0                 0                 0                 0   \n",
       "\n",
       "   FLAG_DOCUMENT_14  FLAG_DOCUMENT_15  FLAG_DOCUMENT_16  FLAG_DOCUMENT_17  \\\n",
       "0                 0                 0                 0                 0   \n",
       "1                 0                 0                 0                 0   \n",
       "2                 0                 0                 0                 0   \n",
       "3                 0                 0                 0                 0   \n",
       "4                 0                 0                 0                 0   \n",
       "\n",
       "   FLAG_DOCUMENT_18  FLAG_DOCUMENT_19  FLAG_DOCUMENT_20  FLAG_DOCUMENT_21  \\\n",
       "0                 0                 0                 0                 0   \n",
       "1                 0                 0                 0                 0   \n",
       "2                 0                 0                 0                 0   \n",
       "3                 0                 0                 0                 0   \n",
       "4                 0                 0                 0                 0   \n",
       "\n",
       "   AMT_REQ_CREDIT_BUREAU_HOUR  AMT_REQ_CREDIT_BUREAU_DAY  \\\n",
       "0                         0.0                        0.0   \n",
       "1                         0.0                        0.0   \n",
       "2                         0.0                        0.0   \n",
       "3                         NaN                        NaN   \n",
       "4                         0.0                        0.0   \n",
       "\n",
       "   AMT_REQ_CREDIT_BUREAU_WEEK  AMT_REQ_CREDIT_BUREAU_MON  \\\n",
       "0                         0.0                        0.0   \n",
       "1                         0.0                        0.0   \n",
       "2                         0.0                        0.0   \n",
       "3                         NaN                        NaN   \n",
       "4                         0.0                        0.0   \n",
       "\n",
       "   AMT_REQ_CREDIT_BUREAU_QRT  AMT_REQ_CREDIT_BUREAU_YEAR  \n",
       "0                        0.0                         1.0  \n",
       "1                        0.0                         0.0  \n",
       "2                        0.0                         0.0  \n",
       "3                        NaN                         NaN  \n",
       "4                        0.0                         0.0  "
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', 3):\n",
    "    # emulating R's `str()` function\n",
    "    print(train.apply(lambda x:[ x.unique()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Preprocessing \n",
    "\n",
    "...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting Binary features to numeric binary features\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert Y/N features to true binary\n",
    "train.FLAG_OWN_CAR = train.FLAG_OWN_CAR.apply(lambda x: (1 if x == \"Y\" else 0))\n",
    "train.FLAG_OWN_REALTY = train.FLAG_OWN_REALTY.apply(lambda x: (1 if x == \"Y\" else 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filling NA values \n",
    "\n",
    "... (of non-categorical variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', 3):\n",
    "    # Count the number of NA values in each column\n",
    "    print(train.apply(lambda x: np.sum(pd.isnull(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[[\"AMT_ANNUITY\", \"AMT_GOODS_PRICE\"]].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill NA values of AMT_ANNUITY and AMT_GOODS_PRICE with -1\n",
    "train.AMT_ANNUITY = train.AMT_ANNUITY.fillna(-1)\n",
    "test.AMT_ANNUITY = test.AMT_ANNUITY.fillna(-1)\n",
    "train.AMT_GOODS_PRICE = train.AMT_GOODS_PRICE.fillna(-1)\n",
    "test.AMT_GOODS_PRICE = test.AMT_GOODS_PRICE.fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.OWN_CAR_AGE.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill NA values of OWN_CAR_AGE with -1\n",
    "train.OWN_CAR_AGE = train.OWN_CAR_AGE.fillna(-1)\n",
    "test.OWN_CAR_AGE = test.OWN_CAR_AGE.fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[[\"EXT_SOURCE_1\", \"EXT_SOURCE_2\", \"EXT_SOURCE_3\"]].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NA values in `EXT_SOURCE_N` columns with -1\n",
    "train.EXT_SOURCE_1 = train.EXT_SOURCE_1.fillna(-1)\n",
    "test.EXT_SOURCE_1 = test.EXT_SOURCE_1.fillna(-1)\n",
    "train.EXT_SOURCE_2 = train.EXT_SOURCE_2.fillna(-1)\n",
    "test.EXT_SOURCE_2 = test.EXT_SOURCE_2.fillna(-1)\n",
    "train.EXT_SOURCE_3 = train.EXT_SOURCE_3.fillna(-1)\n",
    "test.EXT_SOURCE_3 = test.EXT_SOURCE_3.fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = ['APARTMENTS_AVG',\n",
    " 'BASEMENTAREA_AVG',\n",
    " 'YEARS_BEGINEXPLUATATION_AVG',\n",
    " 'YEARS_BUILD_AVG',\n",
    " 'COMMONAREA_AVG',\n",
    " 'ELEVATORS_AVG',\n",
    " 'ENTRANCES_AVG',\n",
    " 'FLOORSMAX_AVG',\n",
    " 'FLOORSMIN_AVG',\n",
    " 'LANDAREA_AVG',\n",
    " 'LIVINGAPARTMENTS_AVG',\n",
    " 'LIVINGAREA_AVG',\n",
    " 'NONLIVINGAPARTMENTS_AVG',\n",
    " 'NONLIVINGAREA_AVG',\n",
    " 'APARTMENTS_MODE',\n",
    " 'BASEMENTAREA_MODE',\n",
    " 'YEARS_BEGINEXPLUATATION_MODE',\n",
    " 'YEARS_BUILD_MODE',\n",
    " 'COMMONAREA_MODE',\n",
    " 'ELEVATORS_MODE',\n",
    " 'ENTRANCES_MODE',\n",
    " 'FLOORSMAX_MODE',\n",
    " 'FLOORSMIN_MODE',\n",
    " 'LANDAREA_MODE',\n",
    " 'LIVINGAPARTMENTS_MODE',\n",
    " 'LIVINGAREA_MODE',\n",
    " 'NONLIVINGAPARTMENTS_MODE',\n",
    " 'NONLIVINGAREA_MODE',\n",
    " 'APARTMENTS_MEDI',\n",
    " 'BASEMENTAREA_MEDI',\n",
    " 'YEARS_BEGINEXPLUATATION_MEDI',\n",
    " 'YEARS_BUILD_MEDI',\n",
    " 'COMMONAREA_MEDI',\n",
    " 'ELEVATORS_MEDI',\n",
    " 'ENTRANCES_MEDI',\n",
    " 'FLOORSMAX_MEDI',\n",
    " 'FLOORSMIN_MEDI',\n",
    " 'LANDAREA_MEDI',\n",
    " 'LIVINGAPARTMENTS_MEDI',\n",
    " 'LIVINGAREA_MEDI',\n",
    " 'NONLIVINGAPARTMENTS_MEDI',\n",
    " 'NONLIVINGAREA_MEDI',\n",
    " 'TOTALAREA_MODE']\n",
    "\n",
    "train[numeric_cols].apply(lambda x: [min(x), max(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in the missing values for test columns with -1\n",
    "for col in numeric_cols:\n",
    "    train[col] = train[col].fillna(-1)\n",
    "    test[col] = test[col].fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_cols = ['OBS_30_CNT_SOCIAL_CIRCLE',\n",
    " 'DEF_30_CNT_SOCIAL_CIRCLE',\n",
    " 'OBS_60_CNT_SOCIAL_CIRCLE',\n",
    " 'DEF_60_CNT_SOCIAL_CIRCLE',\n",
    " 'DAYS_LAST_PHONE_CHANGE',\n",
    " 'AMT_REQ_CREDIT_BUREAU_HOUR',\n",
    " 'AMT_REQ_CREDIT_BUREAU_DAY',\n",
    " 'AMT_REQ_CREDIT_BUREAU_WEEK',\n",
    " 'AMT_REQ_CREDIT_BUREAU_MON',\n",
    " 'AMT_REQ_CREDIT_BUREAU_QRT',\n",
    " 'AMT_REQ_CREDIT_BUREAU_YEAR']\n",
    "\n",
    "train[other_cols].apply(lambda x: [min(x), max(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['OBS_30_CNT_SOCIAL_CIRCLE',\n",
    " 'DEF_30_CNT_SOCIAL_CIRCLE',\n",
    " 'OBS_60_CNT_SOCIAL_CIRCLE',\n",
    " 'DEF_60_CNT_SOCIAL_CIRCLE',\n",
    " 'AMT_REQ_CREDIT_BUREAU_HOUR',\n",
    " 'AMT_REQ_CREDIT_BUREAU_DAY',\n",
    " 'AMT_REQ_CREDIT_BUREAU_WEEK',\n",
    " 'AMT_REQ_CREDIT_BUREAU_MON',\n",
    " 'AMT_REQ_CREDIT_BUREAU_QRT',\n",
    " 'AMT_REQ_CREDIT_BUREAU_YEAR']:\n",
    "    train[col] = train[col].fillna(-1)\n",
    "    test[col] = test[col].fillna(-1)\n",
    "    \n",
    "train.DAYS_LAST_PHONE_CHANGE = train.DAYS_LAST_PHONE_CHANGE.fillna(999)\n",
    "test.DAYS_LAST_PHONE_CHANGE = test.DAYS_LAST_PHONE_CHANGE.fillna(999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', 3):\n",
    "    # Count the number of NA values in each column\n",
    "    print(train.apply(lambda x: np.sum(pd.isnull(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = ['NAME_CONTRACT_TYPE', \n",
    "              'CODE_GENDER', \n",
    "               'NAME_TYPE_SUITE',\n",
    "               'NAME_INCOME_TYPE',\n",
    "               'NAME_EDUCATION_TYPE',\n",
    "               'NAME_FAMILY_STATUS',\n",
    "               'NAME_HOUSING_TYPE',\n",
    "               'OCCUPATION_TYPE',\n",
    "               'HOUR_APPR_PROCESS_START',\n",
    "               'ORGANIZATION_TYPE', \n",
    "               'FONDKAPREMONT_MODE', \n",
    "               'HOUSETYPE_MODE', \n",
    "               'WALLSMATERIAL_MODE', \n",
    "               'EMERGENCYSTATE_MODE'\n",
    "              ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = train[categorical].melt().groupby([\"variable\", \"value\"], as_index = False).size().reset_index()\n",
    "tmp.columns = [\"variable\", \"value\", \"frequency\"]\n",
    "\n",
    "ggplot(tmp, aes(x = \"value\", y = \"frequency\", fill = \"variable\")) +\\\n",
    "    facet_wrap(\"variable\", scales = \"free\") +\\\n",
    "    geom_col(show_legend = False) +\\\n",
    "    theme(axis_text=element_blank())+\\\n",
    "    ggtitle(\"Frequencies of all categorical variables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start with one-hot encoding\n",
    "for col in categorical:\n",
    "    train_encodings, test_encodings = encoder.onehot_encode(train[col],\n",
    "                                                            test[col], \n",
    "                                                           prefix = col.lower() + \"_\")\n",
    "    train = train.merge(train_encodings, left_index = True, right_index = True)\n",
    "    test = test.merge(test_encodings, left_index = True, right_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now add frequency encodings\n",
    "for col in categorical:\n",
    "    train_encodings, test_encodings = encoder.frequency_encode(train[col], \n",
    "                                                          test[col])\n",
    "    train[col.lower() + \"_freq\"] = train_encodings\n",
    "    test[col.lower() + \"_freq\"] = test_encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally, add the mean encodings\n",
    "for col in categorical:\n",
    "    train_encodings, test_encodings = encoder.mean_encode(train.EMERGENCYSTATE_MODE,\n",
    "                                                     train.TARGET, \n",
    "                                                     test.EMERGENCYSTATE_MODE)\n",
    "    train[col.lower() + \"_mean_encoding\"] = train_encodings\n",
    "    test[col.lower() + \"_mean_enccoding\"] = test_encodings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = None\n",
    "display(train[train.columns[122:]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', 3):\n",
    "    # emulating R's `str()` function\n",
    "    print(train.apply(lambda x:[ x.unique()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train.TARGET.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_remove = ['TARGET' , 'SK_ID_CURR'] + categorical\n",
    "all_columns = list(train.columns)\n",
    "for col in to_remove:\n",
    "    all_columns.remove(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(train.columns).remove(\"TARGET\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
